# -*- coding: utf-8 -*-
"""
iCook è‡ªå‹•çˆ¬èŸ² daily.py (2025 Latest-only + start-page ç‰ˆæœ¬)
-------------------------------------------------------------
âœ… åƒ…ä½¿ç”¨ /recipes/latest?page=N
âœ… æ–°å¢ --start-page åƒæ•¸ï¼šå¯æŒ‡å®šå¾ä»»æ„é é–‹å§‹çˆ¬
âœ… è‡ªå‹•åœæ­¢ï¼šè‹¥æ—¥æœŸæ—©æ–¼ --since å³çµæŸ
âœ… æ¯æ—¥è¼¸å‡º CSVï¼šdata/YYYY-MM-DD.csv
"""

import re
import time
import argparse
import os
from datetime import datetime, timedelta, timezone, date
from urllib.parse import urljoin
import requests
import pandas as pd
from bs4 import BeautifulSoup

# --------------------------------------------------
BASE = "https://icook.tw"
LATEST_URL = "https://icook.tw/recipes/latest?page={page}"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}


def http_get(url, timeout=15):
    """å®‰å…¨ HTTP GET"""
    r = requests.get(url, headers=HEADERS, timeout=timeout)
    r.raise_for_status()
    return r.text


def _text(el):
    """å®‰å…¨æ–‡å­—æ“·å–"""
    return el.get_text(" ", strip=True) if el else ""


def daterange_inclusive(start: date, end: date):
    """æ—¥æœŸç¯„åœç”¢ç”Ÿå™¨"""
    d = start
    while d <= end:
        yield d
        d += timedelta(days=1)


# --------------------------------------------------
def parse_list_latest(html: str, debug=False):
    """è§£æ /recipes/latest?page=N é é¢ï¼Œå–å‡ºé£Ÿè­œé€£çµ"""
    soup = BeautifulSoup(html, "lxml")
    links = []
    for a in soup.select('a[href^="/recipes/"]'):
        href = a.get("href", "").strip()
        # ç¢ºä¿æ˜¯é£Ÿè­œé€£çµè€Œé tag
        if re.match(r"^/recipes/\d+$", href):
            full = urljoin(BASE, href)
            if full not in links:
                links.append(full)
                if debug:
                    print(f"  â†³ {full}")
    return links


def parse_recipe_info(html: str, url: str, debug=False):
    """è§£æå–®ç¯‡é£Ÿè­œå…§é ï¼ˆå«ç™¼è¡¨æ—¥æœŸèˆ‡é£Ÿæï¼‰"""
    soup = BeautifulSoup(html, "lxml")
    rid = re.search(r"/recipes/(\d+)", url).group(1)
    title_el = soup.select_one(
        "h1#recipe-name.title") or soup.select_one("h1.title") or soup.select_one("h1")
    title = _text(title_el)
    author = _text(soup.select_one("a.author-name-link"))

    # ç™¼è¡¨æ—¥æœŸ
    time_tag = soup.select_one("time[datetime]")
    if time_tag and time_tag.get("datetime"):
        pub = time_tag["datetime"][:10]
    else:
        pub = ""
    if debug:
        print(f"    [info] {rid} | {pub or 'N/A'} | {title[:40]}")

    ingredients = []
    for li in soup.select("div.recipe-details-ingredients div.ingredients-groups li.ingredient"):
        name = _text(li.select_one(".ingredient-name a.ingredient-search"))
        qty = _text(li.select_one(".ingredient-unit"))
        if name:
            ingredients.append((name, qty))

    return rid, title, author, pub, ingredients


# --------------------------------------------------
def crawl_latest_only(since: str, before: str, start_page: int, outdir="data", sleep=1.5, max_pages=20, debug=False):
    """åƒ…ç”¨ Latest æ¨¡å¼çˆ¬å–ï¼Œæ”¯æ´è‡ªè¨‚èµ·å§‹é """
    os.makedirs(outdir, exist_ok=True)
    since_d = datetime.strptime(since, "%Y-%m-%d").date()
    before_d = datetime.strptime(before, "%Y-%m-%d").date()
    collected = {d.isoformat(): []
                 for d in daterange_inclusive(since_d, before_d)}

    print(f"ğŸ•“ ä½¿ç”¨ iCook /recipes/latest æ¨¡å¼æ“·å–é£Ÿè­œï¼ˆå¾ç¬¬ {start_page} é é–‹å§‹ï¼‰")

    stop_early = False
    oldest_date = None

    for page in range(start_page, start_page + max_pages):
        if stop_early:
            break

        page_url = LATEST_URL.format(page=page)
        try:
            html = http_get(page_url)
        except Exception as e:
            print("[error] ç„¡æ³•è®€å–é é¢:", e)
            continue

        if debug:
            print(f"\n=== ç¬¬ {page} é  ===")
            print(f"ä¾†æº: {page_url}")

        links = parse_list_latest(html, debug)
        if not links:
            print(f"[page {page}] ç„¡æ›´å¤šé£Ÿè­œï¼Œåœæ­¢ã€‚")
            break

        for url in links:
            try:
                rid, title, author, pub, ings = parse_recipe_info(
                    http_get(url), url, debug)
                if not pub:
                    continue

                try:
                    pub_date = datetime.strptime(pub, "%Y-%m-%d").date()
                except Exception:
                    continue

                # è‹¥è¶…é before_d å‰‡ç•¥éï¼ˆå¤ªæ–°ï¼‰
                if pub_date > before_d:
                    continue

                # è‹¥æ—©æ–¼ since_d å‰‡è‡ªå‹•åœæ­¢
                if pub_date < since_d:
                    stop_early = True
                    print(f"â¹ï¸  å·²çˆ¬åˆ°æœ€èˆŠæ—¥æœŸ {pub_date} æ—©æ–¼ {since_d}ï¼Œè‡ªå‹•çµæŸã€‚")
                    break

                rows = []
                if ings:
                    for ing, qty in ings:
                        rows.append(dict(
                            recipe_id=rid,
                            url=url,
                            title=title,
                            author=author,
                            published_date=pub,
                            ingredient=ing,
                            qty_raw=qty,
                            has_ingredient=1,
                            fetched_at=datetime.now(
                                timezone.utc).isoformat(timespec="seconds")
                        ))
                else:
                    rows.append(dict(
                        recipe_id=rid,
                        url=url,
                        title=title,
                        author=author,
                        published_date=pub,
                        ingredient="",
                        qty_raw="",
                        has_ingredient=0,
                        fetched_at=datetime.now(
                            timezone.utc).isoformat(timespec="seconds")
                    ))

                collected[pub].extend(rows)
                oldest_date = pub_date if not oldest_date or pub_date < oldest_date else oldest_date
                if debug:
                    print(
                        f"      [ok] {pub} {title[:25]} ({len(ings)} ingredients)")

                time.sleep(sleep)

            except Exception as e:
                if debug:
                    print("[error]", url, e)
                continue

        time.sleep(sleep)

    # è‹¥ä»æœªé” since_d æç¤ºä½¿ç”¨è€…
    if not stop_early and oldest_date and oldest_date > since_d:
        print(f"âš ï¸ å°šæœªæ‰¾åˆ°æ—©æ–¼ {since_d} çš„é£Ÿè­œï¼Œè«‹æé«˜ --max-pages æˆ–èª¿æ•´ --start-page å†è©¦ã€‚")

    # ä¾æ—¥æœŸè¼¸å‡º CSV
    for d, rows in sorted(collected.items()):
        if not rows:
            continue
        outpath = f"{outdir}/{d}.csv"
        df = pd.DataFrame(rows)
        df.to_csv(outpath, index=False, encoding="utf-8-sig")
        print(f"[saved] {d} -> {outpath} (rows={len(df)})")


# --------------------------------------------------
def main():
    ap = argparse.ArgumentParser(
        description="iCook /recipes/latest æ¨¡å¼çˆ¬èŸ²ï¼ˆæ”¯æ´èµ·å§‹é ï¼‰")
    ap.add_argument("--since", required=True, help="èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)")
    ap.add_argument("--before", required=True, help="çµæŸæ—¥æœŸ (YYYY-MM-DD)")
    ap.add_argument("--start-page", type=int, default=1, help="èµ·å§‹é ç¢¼ï¼ˆé è¨­ç¬¬1é ï¼‰")
    ap.add_argument("--sleep", type=float, default=1.5, help="æ¯ç­†é–“éš”ç§’æ•¸")
    ap.add_argument("--max-pages", type=int,
                    default=20, help="æœ€å¤§é æ•¸ï¼ˆè‡ªå‹•åœæ­¢å¯æå‰çµæŸï¼‰")
    ap.add_argument("--outdir", default="data", help="è¼¸å‡ºè³‡æ–™å¤¾")
    ap.add_argument("--debug", action="store_true", help="å•Ÿç”¨è©³ç´°é™¤éŒ¯è¼¸å‡º")
    args = ap.parse_args()

    crawl_latest_only(args.since, args.before, args.start_page,
                      args.outdir, args.sleep, args.max_pages, args.debug)


if __name__ == "__main__":
    main()
