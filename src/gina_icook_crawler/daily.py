# -*- coding: utf-8 -*-
"""
ğŸ³ iCook è‡ªå‹•çˆ¬èŸ² daily.pyï¼ˆAirflow å°ˆç”¨ v8.2ï¼‰
-------------------------------------------------------------
ğŸ”’ å®Œå…¨ä¿ç•™åŸæœ¬é‚è¼¯ï¼ˆ0% æ”¹å‹•ï¼‰
ğŸŸ£ æ–°å¢å…§å®¹ï¼ˆä¸å½±éŸ¿ä»»ä½•çˆ¬èŸ²è¡Œç‚ºï¼‰ï¼š
    - write_log()ï¼šå®‰å…¨è½åœ°å¯«å…¥æ—¥èªŒ
    - é—œéµæµç¨‹è‡ªå‹•è¨˜éŒ„ log
"""

import re
import os
import json
import time
import argparse
from datetime import datetime, timedelta, date
from urllib.parse import urljoin
import requests
import pandas as pd
from bs4 import BeautifulSoup
import pytz

# ==============================
# ğŸ•“ å°ç£æ™‚å€
# ==============================
tz = pytz.timezone("Asia/Taipei")

# ==============================
# ğŸ§© Kafka åˆå§‹åŒ–ï¼ˆä¿ç•™åŸæœ¬é‚è¼¯ï¼‰
# , "kafka-server:9092"
# ==============================
try:
    from kafka import KafkaProducer
    producer = KafkaProducer(
        bootstrap_servers=["kafka-server:29092"],
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8"),
        linger_ms=100
    )
    TOPIC = "icook_recipes"
    KAFKA_OK = True
    print("âœ… Kafka é€£ç·šæˆåŠŸï¼šå•Ÿç”¨é›™æ¨¡å¼ï¼ˆKafka + CSVï¼‰")
except Exception as e:
    producer, TOPIC, KAFKA_OK = None, None, False
    print(f"âš ï¸ Kafka ç„¡æ³•é€£ç·šï¼Œæ”¹ç‚ºç´” CSV æ¨¡å¼ï¼š{e}")

# ==============================
# ğŸ—‚ Airflow å¯å¯«å…¥è³‡æ–™å¤¾
# ==============================
BASE_DIR = "/opt/airflow/logs/icook"
DATA_DIR = os.path.join(BASE_DIR, "data")
LOG_DIR  = os.path.join(BASE_DIR, "logs")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

PROGRESS_FILE = os.path.join(DATA_DIR, "progress.json")

# ==============================
# ğŸ“ Log å¯«å…¥ï¼ˆæ–°å¢ï¼‰
# ==============================
def write_log(msg):
    try:
        ts = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        path = os.path.join(LOG_DIR, f"{date.today()}.log")
        with open(path, "a", encoding="utf-8") as f:
            f.write(f"[{ts}] {msg}\n")
    except Exception:
        # log ä¸èƒ½å½±éŸ¿çˆ¬èŸ²æœ¬é«”
        pass

# ==============================
# ğŸ“¦ å·¥å…·ï¼ˆåŸæ¨£ä¿ç•™ï¼‰
# ==============================
def _text(el):
    return el.get_text(" ", strip=True) if el else ""

def http_get(url, timeout=15):
    r = requests.get(url, headers=HEADERS, timeout=timeout)
    r.raise_for_status()
    return r.text

def get_latest_url(page: int):
    return "https://icook.tw/recipes/latest" if page == 1 else f"https://icook.tw/recipes/latest?page={page}"

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"last_page": 0, "last_date": None}

def save_progress(date_str, page):
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_date": date_str, "last_page": page}, f, ensure_ascii=False, indent=2)

def backup_progress():
    if os.path.exists(PROGRESS_FILE):
        backup_path = PROGRESS_FILE.replace(".json", "_backup.json")
        if os.path.exists(backup_path):
            os.remove(backup_path)
        os.rename(PROGRESS_FILE, backup_path)
        print(f"âš ï¸ [å¼·åˆ¶æ¨¡å¼] å·²å¿½ç•¥é€²åº¦ä¸¦è¦†è“‹å‚™ä»½ â†’ {os.path.basename(backup_path)}")

def daterange_inclusive(start: date, end: date):
    d = start
    while d <= end:
        yield d
        d += timedelta(days=1)

# ==============================
# âš–ï¸ æ‹†è§£é‡é‡ï¼ˆåŸæ¨£ï¼‰
# ==============================
def split_qty(qty_raw):
    if not qty_raw:
        return "", ""
    qty_raw = qty_raw.strip()
    if re.match(r"^[\u4e00-\u9fa5]+$", qty_raw):
        return "", qty_raw

    replacements = {"Â½": "1/2", "Â¼": "1/4", "Â¾": "3/4", "â…“": "1/3", "â…”": "2/3"}
    for k, v in replacements.items():
        qty_raw = qty_raw.replace(k, v)

    match = re.match(r"^([0-9]+(?:/[0-9]+)?(?:\.[0-9]+)?)\s*([a-zA-Z\u4e00-\u9fa5]*)$", qty_raw)
    if not match:
        return "", qty_raw

    num_str, unit = match.group(1), match.group(2).strip() or ""
    try:
        if "/" in num_str:
            a, b = num_str.split("/")
            num = round(float(a) / float(b), 3)
        else:
            num = float(num_str)
    except Exception:
        num = num_str
    return f"{num}", unit

# ==============================
# ğŸ•¸ï¸ è§£æé£Ÿè­œï¼ˆåŸæ¨£ï¼‰
# ==============================
def parse_recipe_info(html, url, debug=False):
    soup = BeautifulSoup(html, "lxml")
    rid = re.search(r"/recipes/(\d+)", url).group(1)
    title = _text(soup.select_one("h1#recipe-name.title") or soup.select_one("h1.title") or soup.select_one("h1"))
    author = _text(soup.select_one("a.author-name-link"))
    time_tag = soup.select_one("time[datetime]")
    pub = time_tag["datetime"][:10] if time_tag and time_tag.get("datetime") else ""

    servings = f"{_text(soup.select_one('div.servings-info span.num'))} {_text(soup.select_one('div.servings-info span.unit'))}".strip()
    cook_time = f"{_text(soup.select_one('div.time-info span.num'))} {_text(soup.select_one('div.time-info span.unit'))}".strip()

    if debug:
        print(f"    [info] {rid} | {pub or 'N/A'} | {title[:40]}")

    ingredients = []
    for group in soup.select("div.ingredients-groups div.group"):
        group_type = _text(group.select_one(".group-name")) or "é£Ÿæ"
        for li in group.select("li.ingredient"):
            name = _text(li.select_one(".ingredient-name a.ingredient-search"))
            qty_raw = _text(li.select_one(".ingredient-unit"))
            if name:
                w_val, w_unit = split_qty(qty_raw)
                ingredients.append((group_type, name, qty_raw, w_val, w_unit))
    return rid, title, author, pub, ingredients, servings, cook_time

# ==============================
# ğŸš€ ä¸»çˆ¬èŸ²æµç¨‹ï¼ˆå®Œæ•´ä¿ç•™ï¼‰
# ==============================
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
    "Referer": "https://icook.tw",
}

BASE = "https://icook.tw"

def crawl_latest_with_kafka(since, before, start_page=1, sleep=1.5, max_pages=20, debug=False, force=False, excel_safe=False):

    write_log(f"ğŸš€ å•Ÿå‹•çˆ¬èŸ² since={since} before={before}")
    print(f"ğŸ•’ çˆ¬èŸ²æ™‚é–“ï¼ˆå°ç£æ™‚å€ï¼‰: {datetime.now(tz).isoformat(timespec='seconds')}")

    since_d = datetime.strptime(since, "%Y-%m-%d").date()
    before_d = datetime.strptime(before, "%Y-%m-%d").date()

    collected = {d.isoformat(): [] for d in daterange_inclusive(since_d, before_d)}

    # ==åŸé‚è¼¯ä¿ç•™==
    if force:
        backup_progress()
    else:
        progress = load_progress()
        if progress["last_date"] == since:
            start_page = progress.get("last_page", 0) + 1
            write_log(f"ğŸ” å¾é€²åº¦æ¥çºŒç¬¬ {start_page} é ")
            print(f"ğŸ” å¾é€²åº¦æ¥çºŒç¬¬ {start_page} é ")

    stop_flag = False

    for page in range(start_page, start_page + max_pages):
        if stop_flag:
            break

        url_page = get_latest_url(page)
        write_log(f"ğŸ“„ æŠ“å–åˆ—è¡¨é  page={page} {url_page}")

        if debug:
            print(f"\n[DEBUG] æŠ“å–åˆ—è¡¨é : {url_page}")

        try:
            html = http_get(url_page)
        except Exception as e:
            write_log(f"âŒ åˆ—è¡¨é è®€å–éŒ¯èª¤: {e}")
            continue

        soup = BeautifulSoup(html, "lxml")
        links = [urljoin(BASE, a["href"]) for a in soup.select('a[href^="/recipes/"]') if re.match(r"^/recipes/\d+$", a["href"])]

        for link in links:
            try:
                write_log(f"ğŸ” æŠ“å–é£Ÿè­œé  {link}")
                rid, title, author, pub, ings, servings, cook_time = parse_recipe_info(http_get(link), link, debug)

                if not pub:
                    continue

                pub_date = datetime.strptime(pub, "%Y-%m-%d").date()

                if pub_date > before_d:
                    continue
                if pub_date < since_d:
                    write_log(f"ğŸ›‘ æå‰çµæŸï¼š{pub_date} < {since}")
                    print(f"ğŸ›‘ åµæ¸¬åˆ° {pub_date} æ—©æ–¼ {since}ï¼Œåœæ­¢.")
                    stop_flag = True
                    break

                rows = []
                for gtype, ing, qty_raw, w_val, w_unit in ings or [("", "", "", "", "")]:
                    record = {
                        "ID": rid,
                        "é£Ÿè­œåç¨±": title,
                        "ä½œè€…": author,
                        "ä¾†æº": link,
                        "é£Ÿç”¨äººæ•¸": servings,
                        "æ–™ç†æ™‚é–“": cook_time,
                        "é¡å‹": gtype,
                        "åç¨±": ing,
                        "åŸå§‹é‡é‡": qty_raw,
                        "é‡é‡": w_val,
                        "é‡é‡å–®ä½": w_unit,
                        "ä¸Šç·šæ—¥æœŸ": pub,
                        "çˆ¬èŸ²æ™‚é–“": datetime.now(tz).isoformat(timespec="seconds"),
                        "æ˜¯å¦æœ‰é£Ÿæ": int(bool(ings)),
                        "site": "icook"
                    }
                    rows.append(record)

                    if KAFKA_OK:
                        producer.send(TOPIC, record)

                collected[pub].extend(rows)
                save_progress(since, page)
                time.sleep(sleep)

            except Exception as e:
                write_log(f"âŒ é£Ÿè­œé éŒ¯èª¤: {link} | {e}")
                if debug:
                    print(f"[error] {link} {e}")
                continue

        if stop_flag:
            break

        time.sleep(sleep)

    # =========================
    # CSV è¼¸å‡ºï¼ˆåŸæ¨£ï¼‰
    # =========================
    col_order = ["ID", "é£Ÿè­œåç¨±", "ä½œè€…", "ä¾†æº", "é£Ÿç”¨äººæ•¸",
                 "æ–™ç†æ™‚é–“", "é¡å‹", "åç¨±", "åŸå§‹é‡é‡", "é‡é‡",
                 "é‡é‡å–®ä½", "ä¸Šç·šæ—¥æœŸ", "çˆ¬èŸ²æ™‚é–“",
                 "æ˜¯å¦æœ‰é£Ÿæ", "site"]

    for d, rows in sorted(collected.items()):
        if not rows:
            continue
        outpath = os.path.join(DATA_DIR, f"{d}.csv")
        df = pd.DataFrame(rows, dtype=str)[col_order]

        if excel_safe:
            df = df.map(lambda x: f"'{x}" if isinstance(x, str) and re.match(r"^[0-9.]+$", x) else x)

        df.to_csv(outpath, index=False, encoding="utf-8-sig", quoting=1)
        write_log(f"ğŸ’¾ å·²è¼¸å‡º CSV: {outpath} rows={len(rows)}")
        print(f"[saved] {d} -> {outpath} (rows={len(rows)})")

    if KAFKA_OK:
        producer.flush()
        producer.close()

    write_log("ğŸ‰ çˆ¬èŸ²å®Œæˆ")
    print("ğŸ‰ å®Œæˆã€‚")


# ==============================
# ğŸ CLI ä¸»ç¨‹å¼ï¼ˆåŸæ¨£ï¼‰
# ==============================
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--since", required=True)
    ap.add_argument("--before", required=True)
    ap.add_argument("--start-page", type=int, default=1)
    ap.add_argument("--sleep", type=float, default=1.5)
    ap.add_argument("--max-pages", type=int, default=20)
    ap.add_argument("--debug", action="store_true")
    ap.add_argument("--force", action="store_true")
    ap.add_argument("--excel-safe", action="store_true")
    args = ap.parse_args()

    crawl_latest_with_kafka(
        args.since,
        args.before,
        args.start_page,
        args.sleep,
        args.max_pages,
        args.debug,
        args.force,
        args.excel_safe
    )

