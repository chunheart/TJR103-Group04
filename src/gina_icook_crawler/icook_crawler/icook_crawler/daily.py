# -*- coding: utf-8 -*-
"""
ğŸ³ iCook è‡ªå‹•çˆ¬èŸ² daily.pyï¼ˆv7.4 â€” å¼·åŒ–ç‰ˆæ™ºæ…§ç¿»é ï¼‹å°ç£æ™‚å€ï¼‰
-------------------------------------------------------------
âœ… site æ¬„ä½å›ºå®šå€¼ï¼šicookï¼ˆæ–¹ä¾¿å¤šç«™æ•´åˆï¼‰
âœ… CSV æ¬„ä½å›ºå®šé †åºï¼ˆ15 æ¬„ï¼‰
âœ… æ”¯æ´ Excel-safe æ¨¡å¼ï¼ˆé˜²æ­¢ 0.5 â†’ æ—¥æœŸï¼‰
âœ… Kafka / CSV é›™æ¨¡å¼å…±ç”¨
âœ… å°ç£æ™‚å€ï¼ˆAsia/Taipeiï¼‰
âœ… å¼·åŒ–æ™ºæ…§ç¿»é é˜²å‘†ï¼šåªè¦é£Ÿè­œå…§é æ—¥æœŸæ—©æ–¼ since â†’ ç«‹å³åœæ­¢
âœ… å•Ÿå‹•æ™‚å°å‡ºçˆ¬èŸ²æ™‚é–“ï¼ˆå°ç£æ™‚å€ï¼‰
"""

import re
import os
import json
import time
import argparse
from datetime import datetime, timedelta, date
from urllib.parse import urljoin
import requests
import pandas as pd
from bs4 import BeautifulSoup
import pytz  # âœ… å°ç£æ™‚å€æ”¯æ´

# ==============================
# ğŸ•“ å°ç£æ™‚å€è¨­å®š
# ==============================
tz = pytz.timezone("Asia/Taipei")

# ==============================
# ğŸ§© Kafka åˆå§‹åŒ–
# ==============================
try:
    from kafka import KafkaProducer
    producer = KafkaProducer(
        bootstrap_servers=["kafka-server:29092", "kafka-server:9092", "localhost:9092"],
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8"),
        linger_ms=100
    )
    TOPIC = "icook_recipes"
    KAFKA_OK = True
    print("âœ… Kafka é€£ç·šæˆåŠŸï¼šå•Ÿç”¨é›™æ¨¡å¼ï¼ˆKafka + CSVï¼‰")
except Exception as e:
    producer, TOPIC, KAFKA_OK = None, None, False
    print(f"âš ï¸ Kafka ç„¡æ³•é€£ç·šï¼Œæ”¹ç‚ºç´” CSV æ¨¡å¼ï¼š{e}")

# ==============================
# ğŸ—ï¸ åŸºæœ¬è¨­å®š
# ==============================
BASE = "https://icook.tw"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0",
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
    "Referer": "https://icook.tw",
}

if os.name == "nt":
    DATA_DIR = "data"
else:
    DATA_DIR = "/app/data"

PROGRESS_FILE = os.path.join(DATA_DIR, "progress.json")

# ==============================
# ğŸ“¦ å·¥å…·
# ==============================
def _text(el):
    return el.get_text(" ", strip=True) if el else ""

def http_get(url, timeout=15):
    r = requests.get(url, headers=HEADERS, timeout=timeout)
    r.raise_for_status()
    return r.text

def get_latest_url(page: int):
    return "https://icook.tw/recipes/latest" if page == 1 else f"https://icook.tw/recipes/latest?page={page}"

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"last_page": 0, "last_date": None}

def save_progress(date_str, page):
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_date": date_str, "last_page": page}, f, ensure_ascii=False, indent=2)

def backup_progress():
    if os.path.exists(PROGRESS_FILE):
        backup_path = PROGRESS_FILE.replace(".json", "_backup.json")
        if os.path.exists(backup_path):
            os.remove(backup_path)
        os.rename(PROGRESS_FILE, backup_path)
        print(f"âš ï¸ [å¼·åˆ¶æ¨¡å¼] å·²å¿½ç•¥é€²åº¦ä¸¦è¦†è“‹å‚™ä»½ â†’ {os.path.basename(backup_path)}")

def daterange_inclusive(start: date, end: date):
    d = start
    while d <= end:
        yield d
        d += timedelta(days=1)

# ==============================
# âš–ï¸ æ‹†è§£é‡é‡
# ==============================
def split_qty(qty_raw):
    if not qty_raw:
        return "", ""
    qty_raw = qty_raw.strip()
    if re.match(r"^[\u4e00-\u9fa5]+$", qty_raw):
        return "", qty_raw

    replacements = {"Â½": "1/2", "Â¼": "1/4", "Â¾": "3/4", "â…“": "1/3", "â…”": "2/3"}
    for k, v in replacements.items():
        qty_raw = qty_raw.replace(k, v)

    match = re.match(r"^([0-9]+(?:/[0-9]+)?(?:\.[0-9]+)?)\s*([a-zA-Z\u4e00-\u9fa5]*)$", qty_raw)
    if not match:
        return "", qty_raw

    num_str, unit = match.group(1), match.group(2).strip() or ""
    try:
        if "/" in num_str:
            a, b = num_str.split("/")
            num = round(float(a) / float(b), 3)
        else:
            num = float(num_str)
    except Exception:
        num = num_str
    return f"{num}", unit

# ==============================
# ğŸ•¸ï¸ è§£æé£Ÿè­œ
# ==============================
def parse_recipe_info(html, url, debug=False):
    soup = BeautifulSoup(html, "lxml")
    rid = re.search(r"/recipes/(\d+)", url).group(1)
    title = _text(soup.select_one("h1#recipe-name.title") or soup.select_one("h1.title") or soup.select_one("h1"))
    author = _text(soup.select_one("a.author-name-link"))
    time_tag = soup.select_one("time[datetime]")
    pub = time_tag["datetime"][:10] if time_tag and time_tag.get("datetime") else ""

    servings = f"{_text(soup.select_one('div.servings-info span.num'))} {_text(soup.select_one('div.servings-info span.unit'))}".strip()
    cook_time = f"{_text(soup.select_one('div.time-info span.num'))} {_text(soup.select_one('div.time-info span.unit'))}".strip()

    if debug:
        print(f"    [info] {rid} | {pub or 'N/A'} | {title[:40]}")

    ingredients = []
    for group in soup.select("div.ingredients-groups div.group"):
        group_type = _text(group.select_one(".group-name")) or "é£Ÿæ"
        for li in group.select("li.ingredient"):
            name = _text(li.select_one(".ingredient-name a.ingredient-search"))
            qty_raw = _text(li.select_one(".ingredient-unit"))
            if name:
                w_val, w_unit = split_qty(qty_raw)
                ingredients.append((group_type, name, qty_raw, w_val, w_unit))
    return rid, title, author, pub, ingredients, servings, cook_time

# ==============================
# ğŸš€ ä¸»çˆ¬å–æµç¨‹
# ==============================
def crawl_latest_with_kafka(since, before, start_page=1, sleep=1.5, max_pages=20, debug=False, force=False, excel_safe=False):
    print(f"ğŸ•’ çˆ¬èŸ²æ™‚é–“ï¼ˆå°ç£æ™‚å€ï¼‰: {datetime.now(tz).isoformat(timespec='seconds')}")
    os.makedirs(DATA_DIR, exist_ok=True)
    since_d = datetime.strptime(since, "%Y-%m-%d").date()
    before_d = datetime.strptime(before, "%Y-%m-%d").date()
    collected = {d.isoformat(): [] for d in daterange_inclusive(since_d, before_d)}

    if force:
        backup_progress()
    else:
        progress = load_progress()
        if progress["last_date"] == since:
            start_page = progress.get("last_page", 0) + 1
            print(f"ğŸ” å¾é€²åº¦æ¥çºŒç¬¬ {start_page} é ")

    stop_flag = False

    for page in range(start_page, start_page + max_pages):
        if stop_flag:
            break

        url_page = get_latest_url(page)
        if debug:
            print(f"\n[DEBUG] æŠ“å–åˆ—è¡¨é : {url_page}")
        try:
            html = http_get(url_page)
        except Exception as e:
            print(f"[error] ç„¡æ³•è®€å–é é¢: {e}")
            continue

        soup = BeautifulSoup(html, "lxml")
        links = [urljoin(BASE, a["href"]) for a in soup.select('a[href^="/recipes/"]') if re.match(r"^/recipes/\d+$", a["href"])]

        for link in links:
            try:
                rid, title, author, pub, ings, servings, cook_time = parse_recipe_info(http_get(link), link, debug)
                if not pub:
                    continue
                pub_date = datetime.strptime(pub, "%Y-%m-%d").date()
                if pub_date > before_d:
                    continue

                # âœ… å¼·åŒ–ï¼šè‹¥ç™¼ç¾å…§é æ—¥æœŸæ—©æ–¼ since â†’ ç«‹å³åœæ­¢æ•´å€‹çˆ¬èŸ²
                if pub_date < since_d:
                    print(f"ğŸ›‘ åµæ¸¬åˆ° {pub_date} æ—©æ–¼ {since}ï¼Œææ—©åœæ­¢ç¿»é ã€‚")
                    stop_flag = True
                    break

                rows = []
                for gtype, ing, qty_raw, w_val, w_unit in ings or [("", "", "", "", "")]:
                    record = {
                        "ID": rid,
                        "é£Ÿè­œåç¨±": title,
                        "ä½œè€…": author,
                        "ä¾†æº": link,
                        "é£Ÿç”¨äººæ•¸": servings,
                        "æ–™ç†æ™‚é–“": cook_time,
                        "é¡å‹": gtype,
                        "åç¨±": ing,
                        "åŸå§‹é‡é‡": qty_raw,
                        "é‡é‡": w_val,
                        "é‡é‡å–®ä½": w_unit,
                        "ä¸Šç·šæ—¥æœŸ": pub,
                        "çˆ¬èŸ²æ™‚é–“": datetime.now(tz).isoformat(timespec="seconds"),
                        "æ˜¯å¦æœ‰é£Ÿæ": int(bool(ings)),
                        "site": "icook"
                    }
                    rows.append(record)
                    if KAFKA_OK:
                        producer.send(TOPIC, record)
                collected[pub].extend(rows)
                save_progress(since, page)
                time.sleep(sleep)
            except Exception as e:
                if debug:
                    print(f"[error] {link} {e}")
                continue

        if stop_flag:
            break
        time.sleep(sleep)

    col_order = ["ID", "é£Ÿè­œåç¨±", "ä½œè€…", "ä¾†æº", "é£Ÿç”¨äººæ•¸", "æ–™ç†æ™‚é–“",
                 "é¡å‹", "åç¨±", "åŸå§‹é‡é‡", "é‡é‡", "é‡é‡å–®ä½", "ä¸Šç·šæ—¥æœŸ",
                 "çˆ¬èŸ²æ™‚é–“", "æ˜¯å¦æœ‰é£Ÿæ", "site"]

    for d, rows in sorted(collected.items()):
        if not rows:
            continue
        outpath = os.path.join(DATA_DIR, f"{d}.csv")
        df = pd.DataFrame(rows, dtype=str)[col_order]
        if excel_safe:
            df = df.map(lambda x: f"'{x}" if isinstance(x, str) and re.match(r"^[0-9.]+$", x) else x)
            print("ğŸ§¾ æ¨¡å¼ï¼šExcel-safe CSV æ¨¡å¼ï¼ˆé˜²æ­¢ Excel æ—¥æœŸèª¤è½‰ï¼‰")
        else:
            print("ğŸš€ æ¨¡å¼ï¼šä¸€èˆ¬ CSV æ¨¡å¼ï¼ˆæ­£å¼ç‰ˆæœ¬ï¼‰")
        df.to_csv(outpath, index=False, encoding="utf-8-sig", quoting=1)
        print(f"[saved] {d} -> {outpath} (rows={len(rows)})")

    if KAFKA_OK:
        producer.flush()
        producer.close()
    print("ğŸ‰ å®Œæˆã€‚")

# ==============================
# ğŸ ä¸»ç¨‹å¼
# ==============================
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--since", required=True)
    ap.add_argument("--before", required=True)
    ap.add_argument("--start-page", type=int, default=1)
    ap.add_argument("--sleep", type=float, default=1.5)
    ap.add_argument("--max-pages", type=int, default=20)
    ap.add_argument("--debug", action="store_true")
    ap.add_argument("--force", action="store_true")
    ap.add_argument("--excel-safe", action="store_true", help="Excel å®‰å…¨æ¨¡å¼ï¼ˆé˜²æ­¢ 0.5 è¢«è½‰æ—¥æœŸï¼‰")
    args = ap.parse_args()

    crawl_latest_with_kafka(
        args.since,
        args.before,
        args.start_page,
        args.sleep,
        args.max_pages,
        args.debug,
        args.force,
        args.excel_safe
    )

