import pandas as pd
import json
import time
import os
import re

from pathlib import Path
from typing import List, Dict, Optional, Union
from google import genai
from google.genai import types

# ================= CONFIGURATION =================
API_KEY = os.getenv("GEMINI_API_KEY", "è«‹å¡«å…¥API KEY") 
MODEL_NAME = "gemini-2.5-flash"

# æª”æ¡ˆè·¯å¾‘
ROOT_DIR = Path(__file__).resolve().parents[5] # Root dir : project_footprint_calculation
MAPPING_DB_FILE = ROOT_DIR / "data" / "db_unit_normalization" / "unit_normalization_db.csv"

# ================= è½‰æ›è¦å‰‡åº« =================
STANDARD_RULES: Dict[str, float] = {
    "kg": 1000, "å…¬æ–¤": 1000, 
    "g": 1, "å…‹": 1, "å…¬å…‹": 1,
    "æ–¤": 600, "å°æ–¤": 600, 
    "å…©": 37.5, 
    "ç£…": 453.6, "lb": 453.6, 
    "oz": 28.35, "ç›å¸": 28.35,
    "å°‘è¨±": 0.5, "é©é‡": 1.0, "ä¸€å°æ’®": 0.5, "æŠŠ": 30.0,
}

SPECIFIC_RULES: Dict[tuple, float] = {
    ("è›‹", "å€‹"): 50.0, ("é›è›‹", "å€‹"): 50.0, ("å…¨è›‹", "å€‹"): 50.0,
    ("è›‹é»ƒ", "å€‹"): 20.0, ("è›‹ç™½", "å€‹"): 30.0, ("é€£æ®¼é›è›‹", "å€‹"): 65.0, 
    ("B.é›è›‹", "é¡†"): 50.0,
    ("ç™½ç±³", "æ¯"): 145.0, ("ç±³", "æ¯"): 145.0, ("ç³¯ç±³ç²‰", "æ¯"): 120.0,
    ("ç³–", "æ¯"): 200.0, ("ç ‚ç³–", "æ¯"): 200.0, ("ç´°ç ‚ç³–", "æ¯"): 200.0,
    ("éºµç²‰", "æ¯"): 120.0, ("ä½ç­‹éºµç²‰", "æ¯"): 120.0, 
    ("ä¸­ç­‹éºµç²‰", "æ¯"): 120.0, ("é«˜ç­‹éºµç²‰", "æ¯"): 120.0,
    ("æ²¹", "æ¯"): 227.0, ("å¥¶æ²¹", "å¤§åŒ™"): 13.0,
}

VOLUME_TO_ML: Dict[str, float] = {
    "å¤§åŒ™": 15, "tbsp": 15, "T": 15, "åŒ™": 15,
    "å°åŒ™": 5, "tsp": 5, "t": 5, "èŒ¶åŒ™": 5,
    "æ¯": 240, "cup": 240, "C": 240, "ç±³æ¯": 180,
    "ml": 1, "cc": 1, "ã„": 1, "å…¬å‡": 1000, "L": 1000,
    "åˆ1/2æ¯": 360, "åˆ1/2å¤§åŒ™": 22.5
}

class IngredientNormalizer:
    def __init__(self):
        self.client = genai.Client(api_key=API_KEY)
        self.mapping_db = self._load_mapping_db()
        
    def _load_mapping_db(self) -> pd.DataFrame:
        if MAPPING_DB_FILE.exists():
            print(f" è®€å– AI çŸ¥è­˜åº«ï¼š{MAPPING_DB_FILE}")
            try:
                return pd.read_csv(MAPPING_DB_FILE)
            except pd.errors.EmptyDataError:
                pass
        print(" å»ºç«‹æ–°çš„ AI çŸ¥è­˜åº«")
        return pd.DataFrame(columns=['ingredients', 'unit_name', 'grams_per_unit'])

    def _save_mapping_db(self):
        if not self.mapping_db.empty:
            self.mapping_db.to_csv(MAPPING_DB_FILE, index=False, encoding='utf-8-sig')
            # print(f" (å·²è‡ªå‹•å­˜æª”ï¼Œç›®å‰ç´¯ç© {len(self.mapping_db)} ç­†è¦å‰‡)") 

    def _clean_and_parse_json(self, text: str) -> Optional[Dict]:
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pattern = r'```json\s*(.*?)\s*```'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1))
                except: pass
            
            clean_text = text.replace('```json', '').replace('```', '').strip()
            try:
                return json.loads(clean_text)
            except:
                print(f" JSON è§£æå¤±æ•— (å·²ç•¥éæ­¤æ‰¹æ¬¡)")
                return None

    def ask_gemini(self, items_chunk: List[Dict]) -> Optional[Dict]:
        json_str = json.dumps(items_chunk, ensure_ascii=True)
        prompt = f"""
        You are a helper for normalizing recipe ingredient units to grams (g).
        Input Data (JSON): {json_str}
        Please output a JSON object with format: {{ "items": [ {{ "name": "...", "unit": "...", "g_per_unit": float }} ] }}
        Rules:
        1. Estimate weight in grams for 1 unit.
        2. For vague units, use approx values (0.5-2.0).
        3. If unknown, return 0.
        4. You cannot generate Null
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.models.generate_content(
                    model=MODEL_NAME,
                    contents=prompt,
                    config=types.GenerateContentConfig(response_mime_type="application/json")
                )
                return self._clean_and_parse_json(response.text)
            except Exception as e:
                print(f" API Error ({attempt+1}/{max_retries}): {e}")
                time.sleep(2) # é‡è©¦å‰ç¨å¾®ç­‰å¾…
        return None

    def process_csv(self, input_csv: Path, output_csv_path: Path) -> pd.DataFrame:
        print(f"\n Start processing the dataframeï¼š{input_csv}")
        try:
            df = pd.read_csv(input_csv)
        except Exception as e:
            print(f" æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{input_csv}, {e}")
            return

        if 'unit_name' not in df.columns or 'ingredients' not in df.columns:
            print(" CSV æ¬„ä½éŒ¯èª¤")
            return

        candidates = df[df['unit_name'].notna()][['ingredients', 'unit_name']].drop_duplicates()
        existing_db_keys = set(zip(self.mapping_db['ingredients'], self.mapping_db['unit_name']))
        
        unknown_pairs = []
        for _, row in candidates.iterrows():
            name, unit = str(row['ingredients']), str(row['unit_name'])
            
            if unit in STANDARD_RULES or unit in VOLUME_TO_ML: continue
            matched_specific = False
            for (r_n, r_u), _ in SPECIFIC_RULES.items():
                if r_n in name and r_u == unit: 
                    matched_specific = True; break
            if matched_specific: continue

            if (name, unit) in existing_db_keys: continue
            
            unknown_pairs.append({'name': name, 'unit': unit})
        
        print(f" éœ€é€é AI ä¼°ç®—çš„ç‰¹æ®Šçµ„åˆï¼š{len(unknown_pairs)} ç­† (å·²æ‰£é™¤é‡è¤‡èˆ‡å·²çŸ¥è¦å‰‡)")

        # 2. AI æ‰¹æ¬¡è™•ç† (Batch Processing)
        if unknown_pairs:
            # --- ä¿®æ”¹ï¼šä½¿ç”¨è¼ƒå¤§çš„æ‰¹æ¬¡ (60) æ­é…è¼ƒé•·çš„ç­‰å¾… (10s) ä¾†æ‡‰å°å…è²»ç‰ˆé™åˆ¶ ---
            BATCH_SIZE = 30
            print(f"ğŸ¤– é–‹å§‹å‘¼å« {MODEL_NAME} API (æ¯ {BATCH_SIZE} ç­†è‡ªå‹•å­˜æª”)...")
            
            for i in range(0, len(unknown_pairs), BATCH_SIZE):
                batch = unknown_pairs[i:i+BATCH_SIZE]
                print(f"   è™•ç†é€²åº¦: {i+1}/{len(unknown_pairs)}...")
                
                result = self.ask_gemini(batch)
                
                batch_new_records = []
                if result and 'items' in result:
                    for item in result['items']:
                        batch_new_records.append({
                            'ingredients': item.get('name', 'Unknown'),
                            'unit_name': item.get('unit', 'Unknown'),
                            'grams_per_unit': item.get('g_per_unit', 0)
                        })
                
                if batch_new_records:
                    new_df = pd.DataFrame(batch_new_records)
                    if not self.mapping_db.empty:
                         self.mapping_db = pd.concat([self.mapping_db, new_df], ignore_index=True)
                    else:
                         self.mapping_db = new_df
                    
                    self._save_mapping_db() 

                # --- ä¿®æ”¹ï¼šæ¯æ‰¹æ¬¡è™•ç†å¾Œç­‰å¾… 10 ç§’ï¼Œé™ä½ RPM ---
                print("   ç­‰å¾… 10 ç§’ (é¿å… 429 Rate Limit)...")
                time.sleep(10) 

        # 3. æœ€çµ‚è³‡æ–™è½‰æ›
        print(" æ­£åœ¨é€²è¡Œæœ€çµ‚å–®ä½æ›ç®—...")
        if not self.mapping_db.empty:
            ai_mapping = dict(zip(zip(self.mapping_db['ingredients'], self.mapping_db['unit_name']), self.mapping_db['grams_per_unit']))
        else:
            ai_mapping = {}

        def convert_row(row):
            w_str = str(row.get('Weight', 0))
            u = str(row.get('unit_name', ''))
            name = str(row.get('ingredients', ''))
            
            try:
                if pd.isna(row.get('Weight')) or w_str.lower() in ['nan', 'null', '']:
                    w = 1.0 if (u in STANDARD_RULES or u in VOLUME_TO_ML) else 0
                elif '/' in w_str:
                    w = float(eval(w_str))
                else:
                    w = float(w_str)
            except:
                w = 0

            for (r_n, r_u), val in SPECIFIC_RULES.items():
                if r_n in name and r_u == u: return w * val

            if u in STANDARD_RULES: return w * STANDARD_RULES[u]
            
            ai_factor = ai_mapping.get((name, u))
            if ai_factor is not None: return w * ai_factor

            if u in VOLUME_TO_ML: return w * VOLUME_TO_ML[u]
            
            return None

        df['Normalized_Weight_g'] = df.apply(convert_row, axis=1)
        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f" å…¨éƒ¨å®Œæˆï¼çµæœå·²å„²å­˜è‡³ï¼š{output_csv_path}")

def main():
    project_root = Path(__file__).parents[2]
    input_csv = project_root / "data/db_ingredients/icook_recipe_2025-11-19_mydatabase_recipe_ingredients.csv"
    output_csv = project_root / "data/db_ingredients/icook_recipe_2025-11-19_mydatabase_recipe_ingredients_unitN.csv"

    if input_csv.exists():
        normalizer = IngredientNormalizer()
        normalizer.process_csv(input_csv, output_csv)
    else:
        print(f"{input_csv} not found.")

if __name__ == "__main__":
    # main()
    print(ROOT_DIR)
