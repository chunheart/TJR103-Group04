ğŸŒ± Carbon Emission Recipe Pipeline

An automated data pipeline that tracks and analyzes carbon emissions from recipes

ğŸ§© Project Overview

This project aims to build a data-driven carbon emission analysis system based on online recipe data.
The system automatically extracts, processes, and analyzes recipes to estimate their environmental impact.

ğŸ’¡ Key Features

ğŸ•·ï¸ Scrapy for web scraping recipe data

âš™ï¸ Kafka for real-time message streaming

ğŸ—„ï¸ MySQL + MongoDB for raw and staged data storage

ğŸ§  Airflow for orchestration and automation

ğŸ³ Docker for full environment reproducibility

ğŸ§° Poetry for dependency management

ğŸŒ FastAPI / Flask (optional) for web service integration

Produce meaningful insights on sustainable food choices for personal updating

ğŸ” Data Flow Overview
```
flowchart TD
    A[Scrapy] -->|Export| B[CSV]
    A --> C[Kafka Topic]
    C <-->|Producer / Consumer| A
    C --> D[MySQL (raw)]
    D --> E[Transform step]
    E -->|cleaning / normalization / calculation| F[MongoDB (stage)]
    F --> G[Web Service]
```

ğŸ—ï¸ Project Architecture
```
carbon_emission_project/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ carbon_pipeline_dag.py         # Airflow DAG (single-responsibility design)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”‚   â”œâ”€â”€ scrapy_spider.py       # Scrapy spider for recipe data
â”‚   â”‚   â”‚   â””â”€â”€ csv_reader.py
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â”‚   â””â”€â”€ consumer.py
â”‚   â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”‚   â”œâ”€â”€ db_connection.py
â”‚   â”‚   â”‚   â”œâ”€â”€ load_raw.py
â”‚   â”‚   â”‚   â””â”€â”€ load_stage.py
â”‚   â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â”‚   â”œâ”€â”€ normalization.py
â”‚   â”‚   â”‚   â”œâ”€â”€ calculation.py
â”‚   â”‚   â”‚   â””â”€â”€ enrichment.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ logger.py
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ app.py                     # Web API / data visualization
â”‚       â””â”€â”€ api.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ stage/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_kafka.py
â”‚   â”œâ”€â”€ test_load.py
â”‚   â””â”€â”€ test_transform.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ poetry.lock
â””â”€â”€ README.md
```

ğŸ§  Core Design Principles

| Principle                 | Description                                                        |
| ------------------------- | ------------------------------------------------------------------ |
| **Single Responsibility** | Each Airflow task does one thing (easy to debug & retry).          |
| **Stateless Design**      | Every step stores results externally (DB, file) instead of memory. |
| **Retry & Alert**         | Automatic retry on transient errors (Kafka, DB).                   |
| **Unified Logging**       | Centralized logger for consistent Airflow log messages.            |
| **Modularity**            | Each folder handles one part of the ETL pipeline.                  |
| **Dockerized**            | Ensures reproducible local and production environments.            |

âš™ï¸ Tech Stack

| Category             | Technology                             |
| -------------------- | -------------------------------------- |
| **Orchestration**    | Apache Airflow                         |
| **Data Collection**  | Scrapy                                 |
| **Streaming**        | Apache Kafka                           |
| **Storage**          | MySQL (raw data), MongoDB (stage data) |
| **Transformation**   | Python (Pandas / custom logic)         |
| **Packaging**        | Poetry                                 |
| **Containerization** | Docker & docker-compose                |
| **Web Layer**        | Flask / FastAPI (optional)             |
