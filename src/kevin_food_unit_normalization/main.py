import pandas as pd
import json
import time
import os
from pathlib import Path
from typing import List, Dict, Optional, Union
from google import genai
from google.genai import types

# ================= CONFIGURATION =================
# API Key å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé¿å…æ¨é€åˆ° GitHub
API_KEY = os.getenv("GEMINI_API_KEY", "åœ¨æ­¤å¡«å…¥æ‚¨çš„_API_KEY") 
MODEL_NAME = "gemini-2.5-flash"

# æª”æ¡ˆè·¯å¾‘è¨­å®š (ç›¸å°æ–¼æ­¤è…³æœ¬ä½ç½®)
CURRENT_DIR = Path(__file__).parent
MAPPING_DB_FILE = CURRENT_DIR / "unit_mapping_db.csv"

# ================= è½‰æ›è¦å‰‡åº« =================

# 1. General Unit Conversion
# å„ªå…ˆç´šï¼šä½ (ç•¶ç‰¹å®šé£Ÿæè¦å‰‡ç„¡æ³•åŒ¹é…æ™‚ä½¿ç”¨)
STANDARD_RULES: Dict[str, float] = {
    "kg": 1000, "å…¬æ–¤": 1000, 
    "g": 1, "å…‹": 1, "å…¬å…‹": 1,
    "æ–¤": 600, "å°æ–¤": 600, 
    "å…©": 37.5, 
    "ç£…": 453.6, "lb": 453.6, 
    "oz": 28.35, "ç›å¸": 28.35,
    "å°‘è¨±": 0.5, "é©é‡": 1.0, "ä¸€å°æ’®": 0.5, "æŠŠ": 30.0,
}

# 2. Specific Ingredient Rules
# å„ªå…ˆç´šï¼šé«˜ (æœ€ç²¾æº–çš„åŒ¹é…)
# æ ¼å¼: (é£Ÿæé—œéµå­—, å–®ä½): å…¬å…‹æ•¸
SPECIFIC_RULES: Dict[tuple, float] = {
    # --- è›‹é¡ ---
    ("è›‹", "å€‹"): 50.0, ("é›è›‹", "å€‹"): 50.0, ("å…¨è›‹", "å€‹"): 50.0,
    ("è›‹é»ƒ", "å€‹"): 20.0, ("è›‹ç™½", "å€‹"): 30.0, ("é€£æ®¼é›è›‹", "å€‹"): 65.0, 
    ("B.é›è›‹", "é¡†"): 50.0,
    # --- ç±³/ç©€ç‰© ---
    ("ç™½ç±³", "æ¯"): 145.0, ("ç±³", "æ¯"): 145.0, ("ç³¯ç±³ç²‰", "æ¯"): 120.0,
    ("ç³–", "æ¯"): 200.0, ("ç ‚ç³–", "æ¯"): 200.0, ("ç´°ç ‚ç³–", "æ¯"): 200.0,
    ("éºµç²‰", "æ¯"): 120.0, ("ä½ç­‹éºµç²‰", "æ¯"): 120.0, 
    ("ä¸­ç­‹éºµç²‰", "æ¯"): 120.0, ("é«˜ç­‹éºµç²‰", "æ¯"): 120.0,
    ("æ²¹", "æ¯"): 227.0, ("å¥¶æ²¹", "å¤§åŒ™"): 13.0,
}

# 3. Volume to Weight, assume density=1 if unknown
# å„ªå…ˆç´šï¼šæœ€ä½ (ä½œç‚ºæœ€å¾Œæ‰‹æ®µ)
VOLUME_TO_ML: Dict[str, float] = {
    "å¤§åŒ™": 15, "tbsp": 15, "T": 15, "åŒ™": 15,
    "å°åŒ™": 5, "tsp": 5, "t": 5, "èŒ¶åŒ™": 5,
    "æ¯": 240, "cup": 240, "C": 240, "ç±³æ¯": 180,
    "ml": 1, "cc": 1, "ã„": 1, "å…¬å‡": 1000, "L": 1000,
    "åˆ1/2æ¯": 360, "åˆ1/2å¤§åŒ™": 22.5
}

class IngredientNormalizer:
    """
    é£Ÿæå–®ä½æ¨™æº–åŒ–å·¥å…·
    åŠŸèƒ½ï¼šå°‡å„ç¨®éæ¨™æº–å–®ä½ (å¦‚: 1æ¢, 1æ¯, å°‘è¨±) è½‰æ›ç‚ºæ¨™æº–å…¬å…‹æ•¸ (g)ã€‚
    æ©Ÿåˆ¶ï¼šè¦å‰‡æŸ¥è¡¨ -> æ­·å²è³‡æ–™åº« -> Gemini AI ä¼°ç®—
    """
    
    def __init__(self):
        self.client = genai.Client(api_key=API_KEY)
        self.mapping_db = self._load_mapping_db()
        
    def _load_mapping_db(self) -> pd.DataFrame:
        """è®€å–æˆ–åˆå§‹åŒ–æœ¬åœ°çŸ¥è­˜åº« CSV"""
        if MAPPING_DB_FILE.exists():
            print(f"ğŸ“š è®€å– AI çŸ¥è­˜åº«ï¼š{MAPPING_DB_FILE}")
            try:
                return pd.read_csv(MAPPING_DB_FILE)
            except pd.errors.EmptyDataError:
                pass
        
        print(" å»ºç«‹æ–°çš„ AI çŸ¥è­˜åº«")
        return pd.DataFrame(columns=['Ingredient_Name', 'Unit', 'Grams_Per_Unit'])

    def _save_mapping_db(self):
        """å„²å­˜çŸ¥è­˜åº«åˆ° CSV"""
        if not self.mapping_db.empty:
            self.mapping_db.to_csv(MAPPING_DB_FILE, index=False, encoding='utf-8-sig')
            print(f" AI çŸ¥è­˜åº«å·²æ›´æ–°ï¼Œç›®å‰å…±æœ‰ {len(self.mapping_db)} ç­†è¦å‰‡")

    def ask_gemini(self, items_chunk: List[Dict]) -> Optional[Dict]:
        """
        å‘¼å« Gemini API é€²è¡Œå–®ä½ä¼°ç®—
        åŒ…å« Retry æ©Ÿåˆ¶èˆ‡ JSON ç·¨ç¢¼ä¿è­·
        """
        # å¼·åˆ¶ ASCII ç·¨ç¢¼ï¼Œé˜²æ­¢å‚³è¼¸éŒ¯èª¤
        json_str = json.dumps(items_chunk, ensure_ascii=True)
        
        prompt = f"""
        You are a helper for normalizing recipe ingredient units to grams (g).
        Input Data (JSON): {json_str}
        
        Please output a JSON object with format: {{ "items": [ {{ "name": "...", "unit": "...", "g_per_unit": float }} ] }}
        
        Rules:
        1. Estimate the weight in grams for 1 unit of the ingredient.
        2. For volume units (bowl, cup) not in standard list, estimate based on density.
        3. For vague units (pinch, some), use 0.5 to 2.0.
        4. If 'unit' is 'piece/stick/clove' etc., estimate average weight (e.g. 1 cucumber ~ 100g).
        5. If unknown, return 0.
        """
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.models.generate_content(
                    model=MODEL_NAME,
                    contents=prompt,
                    config=types.GenerateContentConfig(response_mime_type="application/json")
                )
                return json.loads(response.text)
            except Exception as e:
                print(f"âš ï¸ API Error (Attempt {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))
                else:
                    print("âŒ API Call Failed after retries.")
                    return None

    def process_csv(self, input_csv_path: Path, output_csv_path: Path):
        """ä¸»è™•ç†æµç¨‹"""
        print(f"\nğŸš€ é–‹å§‹è™•ç†æª”æ¡ˆï¼š{input_csv_path}")
        try:
            df = pd.read_csv(input_csv_path)
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{input_csv_path}")
            return

        # 1. Identify Unknowns
        candidates = df[df['Unit'].notna()][['Ingredient_Name', 'Unit']].drop_duplicates()
        existing_db_keys = set(zip(self.mapping_db['Ingredient_Name'], self.mapping_db['Unit']))
        
        unknown_pairs = []
        for _, row in candidates.iterrows():
            name, unit = str(row['Ingredient_Name']), str(row['Unit'])
            
            # Skip if matches hardcoded rules
            if unit in STANDARD_RULES or unit in VOLUME_TO_ML: continue
            
            matched_specific = False
            for (r_n, r_u), _ in SPECIFIC_RULES.items():
                if r_n in name and r_u == unit: 
                    matched_specific = True; break
            if matched_specific: continue

            # Skip if already in DB
            if (name, unit) in existing_db_keys: continue
            
            unknown_pairs.append({'name': name, 'unit': unit})
        
        print(f"ğŸ“Š éœ€é€é AI ä¼°ç®—çš„ç‰¹æ®Šçµ„åˆï¼š{len(unknown_pairs)} ç­†")

        # 2. AI æ‰¹æ¬¡è™•ç† (Batch Processing)
        if unknown_pairs:
            BATCH_SIZE = 10
            new_records = []
            print(f"ğŸ¤– é–‹å§‹å‘¼å« {MODEL_NAME} API...")
            
            for i in range(0, len(unknown_pairs), BATCH_SIZE):
                batch = unknown_pairs[i:i+BATCH_SIZE]
                print(f"   è™•ç†é€²åº¦: {i+1}/{len(unknown_pairs)}...")
                
                result = self.ask_gemini(batch)
                if result and 'items' in result:
                    for item in result['items']:
                        new_records.append({
                            'Ingredient_Name': item['name'],
                            'Unit': item['unit'],
                            'Grams_Per_Unit': item['g_per_unit']
                        })
                time.sleep(3) # Rate limit buffer

            if new_records:
                new_df = pd.DataFrame(new_records)
                if not self.mapping_db.empty:
                     self.mapping_db = pd.concat([self.mapping_db, new_df], ignore_index=True)
                else:
                     self.mapping_db = new_df
                self._save_mapping_db()

        # 3. Data Normalization
        print(" æ­£åœ¨é€²è¡Œå–®ä½æ›ç®—...")
        ai_mapping = dict(zip(zip(self.mapping_db['Ingredient_Name'], self.mapping_db['Unit']), self.mapping_db['Grams_Per_Unit']))

        def convert_row(row):
            w_str = str(row['Weight'])
            u = str(row['Unit'])
            name = str(row['Ingredient_Name'])
            
            # Parse Weight (Handle fractions and NaN)
            try:
                if pd.isna(row['Weight']) or w_str.lower() in ['nan', 'null', '']:
                    # Default weight=1 for implicit units (e.g., "salt: some")
                    w = 1.0 if (u in STANDARD_RULES or u in VOLUME_TO_ML) else 0
                elif '/' in w_str:
                    w = float(eval(w_str))
                else:
                    w = float(w_str)
            except:
                w = 0

            # Conversion Logic Hierarchy
            # 1. Specific Rules (Most accurate)
            for (r_n, r_u), val in SPECIFIC_RULES.items():
                if r_n in name and r_u == u: return w * val

            # 2. Standard Weight Units
            if u in STANDARD_RULES: return w * STANDARD_RULES[u]
            
            # 3. AI Knowledge Base
            ai_factor = ai_mapping.get((name, u))
            if ai_factor is not None: return w * ai_factor

            # 4. Volume Density Estimation
            if u in VOLUME_TO_ML: return w * VOLUME_TO_ML[u]
            
            return None

        df['Normalized_Weight_g'] = df.apply(convert_row, axis=1)
        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f" è½‰æ›å®Œæˆï¼çµæœå·²å„²å­˜è‡³ï¼š{output_csv_path}")

if __name__ == "__main__":
    # è‡ªå‹•å®šä½å°ˆæ¡ˆæ ¹ç›®éŒ„ (å‡è¨­æ­¤è…³æœ¬åœ¨ src/sub_folder/ ä¸‹)
    project_root = Path(__file__).parents[2]
    
    # è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    input_csv = project_root / "src/kevin_ytower_crawler/ytower_csv_output/ytower_all_recipes.csv"
    output_csv = project_root / "src/kevin_ytower_crawler/ytower_csv_output/ytower_recipes_normalized.csv"
    
    if input_csv.exists():
        normalizer = IngredientNormalizer()
        normalizer.process_csv(input_csv, output_csv)
    else:
        print(f" æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆï¼š{input_csv}")
        print("è«‹ç¢ºèªçˆ¬èŸ²æ˜¯å¦å·²åŸ·è¡Œä¸¦ç”¢ç”Ÿ CSV æª”æ¡ˆã€‚")