import pandas as pd
import json
import time
import os
import re
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Union
from google import genai
from google.genai import types
from kafka import KafkaProducer

# ================= CONFIGURATION =================
# è«‹è¨˜å¾—ç¢ºèªé€™è£¡çš„ API KEY
API_KEY = os.getenv("GEMINI_API_KEY", "è«‹å¡«å…¥API KEY") 
MODEL_NAME = "gemini-2.5-flash"
KAFKA_SERVER = 'kafka:9092'  # Docker å…§éƒ¨ä½¿ç”¨çš„ Kafka ä½å€

# æª”æ¡ˆè·¯å¾‘
CURRENT_DIR = Path(__file__).parent
MAPPING_DB_FILE = CURRENT_DIR / "unit_mapping_db.csv"

# ================= è½‰æ›è¦å‰‡åº«  =================
STANDARD_RULES: Dict[str, float] = {
    "kg": 1000, "å…¬æ–¤": 1000, 
    "g": 1, "å…‹": 1, "å…¬å…‹": 1,
    "æ–¤": 600, "å°æ–¤": 600, 
    "å…©": 37.5, 
    "ç£…": 453.6, "lb": 453.6, 
    "oz": 28.35, "ç›å¸": 28.35,
    "å°‘è¨±": 0.5, "é©é‡": 1.0, "ä¸€å°æ’®": 0.5, "æŠŠ": 30.0,
}

SPECIFIC_RULES: Dict[tuple, float] = {
    ("è›‹", "å€‹"): 50.0, ("é›è›‹", "å€‹"): 50.0, ("å…¨è›‹", "å€‹"): 50.0,
    ("è›‹é»ƒ", "å€‹"): 20.0, ("è›‹ç™½", "å€‹"): 30.0, ("é€£æ®¼é›è›‹", "å€‹"): 65.0, 
    ("B.é›è›‹", "é¡†"): 50.0,
    ("ç™½ç±³", "æ¯"): 145.0, ("ç±³", "æ¯"): 145.0, ("ç³¯ç±³ç²‰", "æ¯"): 120.0,
    ("ç³–", "æ¯"): 200.0, ("ç ‚ç³–", "æ¯"): 200.0, ("ç´°ç ‚ç³–", "æ¯"): 200.0,
    ("éºµç²‰", "æ¯"): 120.0, ("ä½ç­‹éºµç²‰", "æ¯"): 120.0, 
    ("ä¸­ç­‹éºµç²‰", "æ¯"): 120.0, ("é«˜ç­‹éºµç²‰", "æ¯"): 120.0,
    ("æ²¹", "æ¯"): 227.0, ("å¥¶æ²¹", "å¤§åŒ™"): 13.0,
}

VOLUME_TO_ML: Dict[str, float] = {
    "å¤§åŒ™": 15, "tbsp": 15, "T": 15, "åŒ™": 15,
    "å°åŒ™": 5, "tsp": 5, "t": 5, "èŒ¶åŒ™": 5,
    "æ¯": 240, "cup": 240, "C": 240, "ç±³æ¯": 180,
    "ml": 1, "cc": 1, "ã„": 1, "å…¬å‡": 1000, "L": 1000,
    "åˆ1/2æ¯": 360, "åˆ1/2å¤§åŒ™": 22.5
}

class IngredientNormalizer:
    def __init__(self, kafka_server=KAFKA_SERVER):
        self.client = genai.Client(api_key=API_KEY)
        self.mapping_db = self._load_mapping_db()
        self.kafka_producer = self._init_kafka_producer(kafka_server)
        
    def _init_kafka_producer(self, server):
        """åˆå§‹åŒ– Kafka (æ–°å¢åŠŸèƒ½)"""
        try:
            producer = KafkaProducer(
                bootstrap_servers=[server],
                value_serializer=lambda x: json.dumps(x, ensure_ascii=False).encode('utf-8')
            )
            print(f" Kafka é€£ç·šæˆåŠŸï¼š{server}")
            return producer
        except Exception as e:
            print(f" Kafka é€£ç·šå¤±æ•— ({server}) - å°‡åªåŸ·è¡Œé‹ç®—ä¸å‚³é€: {e}")
            return None

    def _load_mapping_db(self) -> pd.DataFrame:
        """(å®Œå…¨ä¿ç•™ä½ çš„åŸå§‹é‚è¼¯)"""
        if MAPPING_DB_FILE.exists():
            try:
                return pd.read_csv(MAPPING_DB_FILE)
            except pd.errors.EmptyDataError:
                pass
        print(" å»ºç«‹æ–°çš„ AI çŸ¥è­˜åº«")
        return pd.DataFrame(columns=['Ingredient_Name', 'Unit', 'Grams_Per_Unit'])

    def _save_mapping_db(self):
        """(å®Œå…¨ä¿ç•™ä½ çš„åŸå§‹é‚è¼¯)"""
        if not self.mapping_db.empty:
            self.mapping_db.to_csv(MAPPING_DB_FILE, index=False, encoding='utf-8-sig')

    def _clean_and_parse_json(self, text: str) -> Optional[Dict]:
        """(å®Œå…¨ä¿ç•™ä½ çš„åŸå§‹é‚è¼¯)"""
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pattern = r'```json\s*(.*?)\s*```'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                try: return json.loads(match.group(1))
                except: pass
            clean_text = text.replace('```json', '').replace('```', '').strip()
            try: return json.loads(clean_text)
            except: return None

    def ask_gemini(self, items_chunk: List[Dict]) -> Optional[Dict]:
        """(å®Œå…¨ä¿ç•™ä½ çš„åŸå§‹é‚è¼¯)"""
        json_str = json.dumps(items_chunk, ensure_ascii=True)
        prompt = f"""
        You are a helper for normalizing recipe ingredient units to grams (g).
        Input Data (JSON): {json_str}
        Please output a JSON object with format: {{ "items": [ {{ "name": "...", "unit": "...", "g_per_unit": float }} ] }}
        Rules:
        1. Estimate weight in grams for 1 unit.
        2. For vague units, use approx values (0.5-2.0).
        3. If unknown, return 0.
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.models.generate_content(
                    model=MODEL_NAME,
                    contents=prompt,
                    config=types.GenerateContentConfig(response_mime_type="application/json")
                )
                return self._clean_and_parse_json(response.text)
            except Exception as e:
                print(f" API Error ({attempt+1}/{max_retries}): {e}")
                time.sleep(2)
        return None

    def process_pipeline(self, input_csv_path: Path, kafka_topic: str, source_name: str):
        """
        ä¸»æµç¨‹ï¼šè®€å– CSV -> AI è£œå…¨ -> è¨ˆç®—é‡é‡ -> å¯«å…¥ Kafka
        """
        print(f"\n é–‹å§‹è™•ç† Pipeline")
        print(f" ä¾†æºæª”æ¡ˆ: {input_csv_path}")
        
        try:
            df = pd.read_csv(input_csv_path)
        except FileNotFoundError:
            print(f" æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{input_csv_path}")
            return

        if 'Unit' not in df.columns or 'Ingredient_Name' not in df.columns:
            print(" CSV æ¬„ä½éŒ¯èª¤")
            return

        # === ç¬¬ä¸€éšæ®µï¼šAI è£œå…¨ (å®Œå…¨ä¿ç•™ä½ çš„åŸå§‹é‚è¼¯) ===
        candidates = df[df['Unit'].notna()][['Ingredient_Name', 'Unit']].drop_duplicates()
        existing_db_keys = set(zip(self.mapping_db['Ingredient_Name'], self.mapping_db['Unit']))
        
        unknown_pairs = []
        for _, row in candidates.iterrows():
            name, unit = str(row['Ingredient_Name']), str(row['Unit'])
            if unit in STANDARD_RULES or unit in VOLUME_TO_ML: continue
            
            matched_specific = False
            for (r_n, r_u), _ in SPECIFIC_RULES.items():
                if r_n in name and r_u == unit: 
                    matched_specific = True; break
            if matched_specific: continue

            if (name, unit) in existing_db_keys: continue
            unknown_pairs.append({'name': name, 'unit': unit})
        
        print(f" éœ€é€é AI ä¼°ç®—çš„ç‰¹æ®Šçµ„åˆï¼š{len(unknown_pairs)} ç­†")

        if unknown_pairs:
            BATCH_SIZE = 30
            print(f"ğŸ¤– é–‹å§‹å‘¼å« Gemini API (å…± {len(unknown_pairs)} ç­†)...")
            for i in range(0, len(unknown_pairs), BATCH_SIZE):
                batch = unknown_pairs[i:i+BATCH_SIZE]
                result = self.ask_gemini(batch)
                
                batch_new_records = []
                if result and 'items' in result:
                    for item in result['items']:
                        batch_new_records.append({
                            'Ingredient_Name': item.get('name', 'Unknown'),
                            'Unit': item.get('unit', 'Unknown'),
                            'Grams_Per_Unit': item.get('g_per_unit', 0)
                        })
                
                if batch_new_records:
                    new_df = pd.DataFrame(batch_new_records)
                    self.mapping_db = pd.concat([self.mapping_db, new_df], ignore_index=True) if not self.mapping_db.empty else new_df
                    self._save_mapping_db()
                
                time.sleep(2) 

        # === ç¬¬äºŒéšæ®µï¼šè¨ˆç®—èˆ‡å‚³è¼¸ (æ”¹ç‚ºè¿´åœˆä»¥æ”¯æ´ Kafka) ===
        print(" æ­£åœ¨é€²è¡Œå–®ä½æ›ç®—ä¸¦å¯«å…¥ Kafka...")
        
        if not self.mapping_db.empty:
            ai_mapping = dict(zip(zip(self.mapping_db['Ingredient_Name'], self.mapping_db['Unit']), self.mapping_db['Grams_Per_Unit']))
        else:
            ai_mapping = {}

        count = 0
        for _, row in df.iterrows():
            # --- [æ ¸å¿ƒé‚è¼¯é–‹å§‹] ---
            w_str = str(row.get('Weight', 0))
            u = str(row.get('Unit', ''))
            name = str(row.get('Ingredient_Name', ''))
            
            try:
                if pd.isna(row.get('Weight')) or w_str.lower() in ['nan', 'null', '']:
                    w = 1.0 if (u in STANDARD_RULES or u in VOLUME_TO_ML) else 0
                elif '/' in w_str: w = float(eval(w_str))
                else: w = float(w_str)
            except: w = 0

            normalized_g = 0.0
            found = False
            
            # å„ªå…ˆé †åº 1: ç‰¹æ®Šè¦å‰‡ (Specific Rules)
            for (r_n, r_u), val in SPECIFIC_RULES.items():
                if r_n in name and r_u == u: 
                    normalized_g = w * val
                    found = True
                    break
            
            if not found:
                # å„ªå…ˆé †åº 2: æ¨™æº–å–®ä½ (Standard Rules)
                if u in STANDARD_RULES:
                    normalized_g = w * STANDARD_RULES[u]
                # å„ªå…ˆé †åº 3: AI çŸ¥è­˜åº« (AI Mapping)
                elif (name, u) in ai_mapping:
                    normalized_g = w * ai_mapping[(name, u)]
                # å„ªå…ˆé †åº 4: å®¹ç©å–®ä½ (Volume Rules)
                elif u in VOLUME_TO_ML:
                    normalized_g = w * VOLUME_TO_ML[u]
                else:
                    normalized_g = None 

            # æº–å‚™å¯«å…¥è³‡æ–™
            data_row = row.to_dict()
            data_row['Normalized_Weight_g'] = normalized_g
            data_row['data_source'] = source_name # æ–°å¢ï¼šä¾†æºæ¨™è¨˜

            # å¯«å…¥ Kafka
            if self.kafka_producer:
                self.kafka_producer.send(kafka_topic, value=data_row)
                count += 1
                if count % 50 == 0:
                    print(f" å·²å‚³é€ {count} ç­†...")

        if self.kafka_producer:
            self.kafka_producer.flush()
            self.kafka_producer.close()
        
        print(f" å…¨éƒ¨å®Œæˆï¼å…±å¯«å…¥ {count} ç­†è³‡æ–™åˆ° Topic: {kafka_topic}")

if __name__ == "__main__":
    # æ¥æ”¶ Airflow å‚³ä¾†çš„åƒæ•¸
    parser = argparse.ArgumentParser(description='é€šç”¨é£Ÿææ­£è¦åŒ–å·¥å…·')
    parser.add_argument('--input', required=True, help='è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--topic', required=True, help='è¦å¯«å…¥çš„ Kafka Topic')
    parser.add_argument('--source', required=True, help='è³‡æ–™ä¾†æºæ¨™è¨˜ (å¦‚: ytower, icook)')
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if input_path.exists():
        normalizer = IngredientNormalizer()
        normalizer.process_pipeline(input_path, args.topic, args.source)
    else:
        print(f" éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ {input_path}")